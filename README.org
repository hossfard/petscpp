* Synopsis
  Lightweight =c++= wrapper for =PETSc=

* Build
  (tested with =petsc-3.4.4=) Consider building =PETSc= with an
  optimized linear algebra system (see example below). An optimized LA
  system can lead to a considerable performance boost.

** Install MPICH
  1. Install =MPICH=
     #+BEGIN_EXAMPLE
     # yum install mpich2
     #+END_EXAMPLE
  2. Load the MPI module
     #+BEGIN_EXAMPLE
     $ module load mpi/mpich-x86_64
     #+END_EXAMPLE
     You can add the above line to =~/.bash_profile= so that MPI
     module is loaded on login
** Build Petsc
*** Get ATLAS
  1. Download =ATLAS= (tested with =atlast-3.10.3=)
  2. Build =ATLAS=
     - Boot the OS with =intel_pstate= disabled. This can be achieved
       by appending ~intel_pstate=disable~ to boot options
     - Disable CPU frequency scaling
       #+BEGIN_EXAMPLE
       $ echo performance > /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor
       $ echo performance > /sys/devices/system/cpu/cpu1/cpufreq/scaling_governor
       ...
       #+END_EXAMPLE
  3. Configure =ATLAS=. Example build process
     #+BEGIN_EXAMPLE
     $ cd /path/to/atlas/dir
     $ mkdir name_of_build_directory && cd name_of_build_directory
     $ ../configure --with-netlib-lapack-tarfile=/path/to/lapak/tarbal.tgz -Fa alg -fPIC
     $ make build
     $ make check
     $ make ptcheck
     $ make time
     #+END_EXAMPLE
*** Build Petsc
  1. Configure and build =PETSc= (tested with =petsc-3.4.4=)
     #+BEGIN_EXAMPLE
     $ ./configure --with-c-support=1 CXXOPTFLAGS=-O3 FOPTFLAGS=-O3 \
           --with-pic --with-debugging=0
           --with-blas-lapack-dir=/path/to/atlast/build/dir/lib \
           --with-x=0 --with-superlu=1 --download-superlu \
           --with-superlu_dist=1 --download-superlu_dist \
           --with-parmetis=1 --download-parmetis --with-metis=1 \
           --download-metis --with-dynamic-loading=0 --with-shared-libraries=1 \
           --download-mpich
     $ make PETSC_DIR=/opt/petsc-3.4.4 PETSC_ARCH=arch-linux2-c-opt all
     #+END_EXAMPLE
  2. Set needed PETSc environmental variables
     #+BEGIN_EXAMPLE
     $ export PETSC_DIR=/path/to/petsc/dir
     $ export PETSC_ARCH=/path/to/petsc/arch/build
     #+END_EXAMPLE
     You can add the above line to =~/.bashrc=
** Build Petscpp
  1. Install =cmake=
     #+BEGIN_EXAMPLE
     # yum install cmake
     #+END_EXAMPLE
  2. Download =Eigen=
  3. Set =Eigen= directory as an env variable
     #+BEGIN_EXAMPLE
     $ export EIGEN_DIR=/path/to/eigen/root/dir
     #+END_EXAMPLE
     (Store in =~/.bashrc=)
  4. Build =petscpp=
     #+BEGIN_EXAMPLE
     $ cd /path/to/petscpp/root/dir/
     $ mkdir build && cd build
     $ cmake ..
     $ make
     #+END_EXAMPLE
  5. Run tests
     #+BEGIN_EXAMPLE
     $ cd /path/to/petscpp/root/dir
     $ cd build/test
     $ ./runall number_of_proc
     #+END_EXAMPLE
  6. Run demos with =mpiexec=
     #+BEGIN_EXAMPLE
     $ cd /path/to/executable
     $ mpiexec -n number_of_procs ./name_of_executable
     #+END_EXAMPLE
     (Limit the processor number up to the number of available
     physical cores when using MPI)
